# Lab 1 Draft

## Robot Platform

Through the semester, you and your group have a choice between two hardware platforms:

1. LoCoBot
2. Turtlebot 3 Waffle Pi

## Gazebo Simulation

<aside>
💡 Finish the simulation on your PC(workstation/laptop).

</aside>

1. Download and execute setup script
    
    ```bash
    git clone https://github.com/Harvard-REACT/Harvard_CS286.git
    cd  Harvard_CS286
    chmod +x setup_hack_1.sh
    ./setup_hack_1.sh
    source ~/.bashrc
    ```
    
2. Launch the **Gazebo** simulation
    
    ```bash
    roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch
    ```
    
    After this step, you’ll see a graphical simulator launched like this. 
    
    ![Untitled](Lab%201%20Draft%20d1a8423a81e84211bf3fba4b5af63015/Untitled.png)
    
    **Gazebo**
    
    > It’s 3D dynamic simulator with the ability to accurately and efficiently simulate populations of robots in complex indoor and outdoor environments.While similar to game engines, Gazebo offers physics simulation at a much higher degree of fidelity, a suite of sensors, and interfaces for both users and programs.
    > 
    
    Using Gazebo, we’re able to emulate the physics of robots and different sensors to some extent. In this mini hack session, we’ll just use this simulator with a toy example. Feel free to hack it!
    
3. Start up the ROS driver of the turtlebot. 
    
    `roslaunch turtlebot3_bringup turtlebot3_remote.launch`
    
4. Start another terminal and list all topics `rostopic list`. You’ll see a list of topics summoned by `turtlebot3_bringup` like this. You might see some additional topics beyond these. 
    
    ![Untitled](Lab%201%20Draft%20d1a8423a81e84211bf3fba4b5af63015/Untitled%201.png)
    

5. Let’s start some keystroke controls using `turtlebot3_teleop`. 

`roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch`

Then, by using keystroke. you’ll see the robot start moving. To this point, you’ll able to almost operate this robot like an actual robot. While it’s moving, check the message published from odom topic and interpret the structure of data 

1. Instead of using the `turtlebot3_teleop`, you can also directly publish message to control topic. Now stop your turtlebot3_teleop with ctrl-c, directly publish messages to topic /cmd_vel like this: 
    
    <aside>
    💡 You don’t need to manually type every character of this command. After tying in `/cmd_vel`, use `tab`to let terminal auto-complete the rest of command.
    
    </aside>
    
    ```bash
    rostopic pub /cmd_vel geometry_msgs/Twist "linear:
      x: 0.1
      y: 0.0
      z: 0.0
    angular:
      x: 0.0
      y: 0.0
      z: 0.0"
    ```
    
    **linear** stands for the linear speed in x,y,z. For now, you only need to concern x of linear speed. Angular here means the angular speed along each axis. So we only need to change the z component of angular speed with ground platforms. 
    

After playing with simulator, we’ll move to our hardware platforms.

## Hardware Setup

### PC Setup

<aside>
💡 Finish this part on your PC

</aside>

1. Network configuration
    1. Connect your laptop or lab workstation to our internal wireless network: 
        
        WiFi Name: CS286-Local-5G
        
        Password: InstaL@b2022
        
    2. Check your host machine’s assigned ip address by using `ifconfig`. It’s possible that this command is unavailable, then use  `ip addr` instead. The assigned ip should always have the format of *192.168.1.xx*
        
        ![Untitled](Lab%201%20Draft%20d1a8423a81e84211bf3fba4b5af63015/Untitled%202.png)
        
    3. Open your .bashrc file and add the corresponding ROS IP setting. 
        
        `gedit ~/.bashrc`
        
        There are two variables you need to add at the bottom of `.bashrc`: `ROS_MASTER_URI` and `ROS_HOSTNAME`
        
        `ROS_MASTER_URI` should start with `http://` and ends with `:11311` as port number. The ip address in between should be your PC’s ip address which will be used as ros master. Keep in mind that you’ll also set it to your PC’s ip address on your remote robot because you want all the robots/desktop agree that there is a master to manage everything. 
        
        The `ROS_HOSTNAE` should be always set to the assigned ip address of your current machine. A schematic is provided below for your understanding. You can also find more information [here](http://wiki.ros.org/ROS/NetworkSetup). ****
        
        ![Untitled](Lab%201%20Draft%20d1a8423a81e84211bf3fba4b5af63015/Untitled%203.png)
        
        ![Untitled](Lab%201%20Draft%20d1a8423a81e84211bf3fba4b5af63015/Untitled%204.png)
        
    4. `source ~/.bashrc` to load the variables in current session. It will be automatically loaded next time you open a terminal. 
    5. Run `roscore` on your PC.

### Bring up your robot(Turtlebot 3 Waffle)

<aside>
💡 Finish this part in terminal by **ssh** into your robot. 
PUT YOUR ROBOT ON THE FLOOR, ALWAYS!!

</aside>

1. Connect the battery and power your robot. 
    
    Voltage checker should be connected firstly to check if battery normally function. The safe range is [11.1V, 12.6V]. It will beep loudly anytime the voltage dropped below 11.1V. After checking the voltage, the battery should be secured as shown below. Then, push the power switch to the right to switch on the robot. 
    

![Untitled](Lab%201%20Draft%20d1a8423a81e84211bf3fba4b5af63015/Untitled%205.png)

1. Obtain the ip address, the hostname and password labeled on your assigned robot. 
2. ssh to the robot through your machine’s terminal(you might need to wait roughly 2 mins until the robot connects to the lab WiFi.
    
    `ssh hostname@ipaddress` 
    
     Example: ssh all_waffle1@192.168.1.4 
    
3. Set the `ROS_MASTER_URI` and `ROS_HOSTNAME` in robot’s .bashrc as we mentioned in previous network setting section.
    
    `echo “export ROS_MASTER_URI=http://yourmasterup:11311” >> ~/.bashrc`
    
4. `roslaunch turtlebot3_bringup turtlebot3_robot.launch`, output would be like this 
    
    ![Untitled](Lab%201%20Draft%20d1a8423a81e84211bf3fba4b5af63015/Untitled%206.png)
    

### Bring up your robot(LoCoBot)

<aside>
💡 PUT YOUR ROBOT ON THE GROUND!

</aside>

1. LoCoBot’s battery has been connected. Simply switch on the on-board computer and the Base. 
    
    ![image (7).png](Lab%201%20Draft%20d1a8423a81e84211bf3fba4b5af63015/image_(7).png)
    
    ![image (8).png](Lab%201%20Draft%20d1a8423a81e84211bf3fba4b5af63015/image_(8).png)
    
2. Obtain the ip address, the hostname and password labeled on your assigned robot. 
3. ssh to the robot through your machine’s terminal(you might need to wait roughly 2 mins until the robot connects to the lab WiFi.
    
    `ssh hostname@ipaddress` 
    
     Example: ssh all_waffle1@192.168.1.4 
    
4. Set the `ROS_MASTER_URI` and `ROS_HOSTNAME` in robot’s .bashrc as instructed in previous [section](https://www.notion.so/Lab-1-Draft-ce698110300843e8929d5266b1e049a0). 
5. load the variables into your current ssh session
    
    `source ~/.bashrc`
    

> Make sure `roscore` is still running on your PC.
> 
1. `roslaunch interbotix_xslocobot_control xslocobot_control.launch`, output would be like this PICTURE NEEDED

## Basic Control and Perception

<aside>
💡 Finish this part on your PC. Make sure you can see all topics being published by the robot’s package on your local PC using `rostopic list`

</aside>

On your local PC, open another terminal

1. As you’ve done in the simulation, you can directly publish the geometry_msg `Twist` to speed control topic `/cmd_vel` like this: 

<aside>
💡 For turtlebot3, the topic name is `/cmd_vel`, 
For LoCoBot, the topic name is `/XXX`.

</aside>

```bash
rostopic pub /cmd_vel geometry_msgs/Twist "linear:
  x: 0.1
  y: 0.0
  z: 0.0
angular:
  x: 0.0
  y: 0.0
  z: 0.0"
```

**linear** stands for the linear speed in x,y,z. For now, you only need to concern x of linear speed. Angular here means the angular speed along each axis. So we only need to change the z component of angular speed with ground platforms. 

### Use waypoint script

1. Rather than directly using velocity control, we also provide a waypoint control script. 
    
    For LoCoBot: `rosrun cs286_mini_hack_1 locobot_pointop_key.py`
    
    For turtlebot: `rosrun cs286_mini_hack_1 turtlebot3_pointop_key.py`
    
2. The program will prompt a target location, try different locations. The target location is always with respect to the initial location where the robot is initially launched.  

### LIDAR scanning and obstacle finding.

Once you bring up the driver, the LIDAR sensor on the top of robot would also start spinning. It will measure obstacles’ distance for each direction(360 degrees)

1. Check the message published in /scan topic, try to interpret the its format. 
2. We provide a script which will stop the robot whenever there is an obstacle in front. Make sure there is nothing in front of the robot.
    
    For LoCoBot:`rosrun cs286_mini_hack_1 locobot_obstacle.py`
    
    For turtlebot:`rosrun cs286_mini_hack_1 turtlebot3_obstacle.py`
    
3. Assumably, once the robot will start moving. it will stop if the obstacle is 0.4m away from the LIDAR sensor(spinning thing 🌀) **not** the robot’s body. 

### Visual Inertial Odometry Sensor

Advances in robotics vision has inspired a regime of works dedicated to use camera as an egomotion estimation sensor. The main objective is to use camera and IMU to process the trajectories of robots. One off-the-shelf solution is the Intel Realsense T265 Camera. It provides a direct estimate of robots’ position and orientation.

On provided robot platform, you’ll find a camera like this

![Untitled](Lab%201%20Draft%20d1a8423a81e84211bf3fba4b5af63015/Untitled%207.png)

We’ve installed all drivers on each robot’s computer. Run this command to bring up the driver

`roslaunch realsense2_camera rs_t265.launch` 

One topic of interest published by the camera node is `/camera/odom/sample` which publish the position and velocity of the robot. 

1. Publish velocity command through corresponding velocity topic for your robot platform.
2. Use `rostopic echo` to check the message from `/camera/odom/sample` and see if the estimated position changes as robot moves. 
3. Open RVIZ from command line, change the fixed frame in global option to `camera_odom_link`, then add the topic `/camera/odom/sample` to display.

## Deliverable

### Autonomous navigation using on-board odometry:

1. **Task:** Navigate the robot autonomously in a square using odom topic “<topic name>”. The robot should complete moving in the entire square without requiring any manual inputs to the script.
    1. You can leverage the waypoint script for this.
    2. Waypoints can be hardcoded beforehand.
2. **Submission files:**
    1. Create a python plot to visualize the trajectory after the robot has completed its waypoints (this can be a separate python script and can be done offline as well).
    2. Collect a bag file of the following topics when the robot is performing the task:
        1. For LoCoBot: odom, /locobot/scan, tf
        2. For Turtlebot: odom, scan, tf
    3. Record a video of your robot performing the task in the testbed.
    4. Save all your code files, rosbag and video files in a folder called “mh1_task1”.

### Autonomous navigation using T265 visual odometer:

1. **Task:** Navigate the robot autonomously in a square using T265 odom topic “/camera/sample/odom”. The robot should complete moving in the entire square without requiring any manual inputs to the script.
    1. You can leverage the waypoint script for this.
    2. Waypoints can be hardcoded beforehand.
2. **Submission files:**
    1. Create a python plot to visualize the trajectory after the robot has completed its waypoints (this can be a separate python script and can be done offline as well).
    2. Create a python plot to overlay the trajectory from T265 visual odometer and the on-board odometer which you collected in task 1. Which one do you think is more accurate?
    3. Collect a bag file of the following topics when the robot is performing the task:
        1. For LoCoBot: /camera/sample/odom, /locobot/scan, tf
        2. For Turtlebot: /camera/sample/odom, scan, tf
    4. Record a video of your robot performing the task in the testbed.
    5. Save all your code files, rosbag and video files in a folder called “mh1_task2”.

### Autonomous navigation in presence of obstacles

1. **Task:** Reuse the code that you developed in task1 or task2 to move the robot in along a square trajectory, however the robot should stop when an obstacle is detected 0.3 m in front of it. Once the obstacle is removed, the robot should continue its waypoint following.
    1. You can leverage the pointop_key.py script for this.
    2. You can leverage the obstacle.py script for this.
    3. Waypoints can be hardcoded beforehand.

**Hint:** A good way for coordinating different nodes (e.g., the pointop_key.py and obstacle.py) is leverage ROS topics to pass information like if there is an object or not in front of the robot.

1. **Submission files:**
    1. Record a video of your robot performing the task in the testbed, where at least 3 obstacles are introduced (and then removed once the robot stops) at different points along robot’s trajectory.
    2. Save all your code and video files in a folder called “mh1_task3”.

### Evasive action when obstacle is detected

1. **Task:** When an obstacle is detected, the robot should find a way to avoid colliding with it and continue moving.
    1. You can leverage the obstacle.py script for this.
    2. The robot should not backup (move in reverse) to avoid collision.
2. **Submission files:**
    1. Record a video of your robot performing the evasive action.
    2. Save all your code and video files in a folder called “mh1_task4”.

## **Submission:**

Zip all the folders -  mh1_task1, mh1_task2, mh1_task3 and a text file with names and IDs of all the group members, in a zip file named “mh1_<group_name>.zip.” Submit the zip file on canvas. Only 1 team member needs to submit it.

## FAQ/debugging